{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7f6cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys; sys.path.append(\"../\")\n",
    "import tiktoken\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5508753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.memmap('../data/openwebtext/train.bin', dtype=np.uint16, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f815d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9035582489,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce59a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8585\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1183071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
    "    ids.append(enc.eot_token) # add the end of text token, e.g. 50256 for gpt2 bpe\n",
    "    # note: I think eot should be prepended not appended... hmm. it's called \"eot\" though...\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5b7aca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [15496, 995, 50256], 'len': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process({'text':\"Hello world\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5800c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "model = GPT.from_pretrained(\"gpt2\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b4f90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = None\n",
    "wpe = None\n",
    "for name, param in model.named_parameters():\n",
    "    if name == 'transformer.wte.weight':\n",
    "        wte = param\n",
    "    elif name == 'transformer.wpe.weight':\n",
    "        wpe = param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a55e913",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'out_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m c_attn \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_attn_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_attn_bias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'out_features'"
     ]
    }
   ],
   "source": [
    "c_attn = nn.Linear(c_attn_weight, bias=c_attn_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "215bbcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wte_embed = nn.Embedding(50257, 768)\n",
    "wpe_embed = nn.Embedding(50257, 768)\n",
    "wte_embed.weight = wte\n",
    "wpe_embed.weight = wpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f36f3a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wte_embed(torch.tensor([15496, 995, 50256])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "wte_OWT = wte_embed(torch.from_numpy((train_data).astype(np.int64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcffa151",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpe_OWT = wpe_embed(torch.from_numpy((train_data).astype(np.int64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299744e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wte_OWT.shape, wpe_OWT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d3872b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
